{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "CLdJR_FM16sP"
   },
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image, ImageEnhance, ImageOps, ImageFile\n",
    "import numpy as np\n",
    "import random\n",
    "import threading, os, time\n",
    "import logging\n",
    "import cv2\n",
    "# logger = logging.getLogger(__name__)\n",
    "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "class DataAugmentation:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def openImage(image):\n",
    "        return Image.open(image, mode=\"r\")\n",
    "\n",
    "    @staticmethod\n",
    "    def randomRotation(image, label, mode=Image.BICUBIC):\n",
    "\n",
    "        random_angle = np.random.randint(1, 360)\n",
    "        return image.rotate(random_angle, mode), label.rotate(random_angle, Image.NEAREST)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def randomCrop(image, label):\n",
    "\n",
    "        image_width = image.size[0]\n",
    "        image_height = image.size[1]\n",
    "        crop_win_size = np.random.randint(40, 68)\n",
    "        random_region = (\n",
    "            (image_width - crop_win_size) >> 1, (image_height - crop_win_size) >> 1, (image_width + crop_win_size) >> 1,\n",
    "            (image_height + crop_win_size) >> 1)\n",
    "        return image.crop(random_region), label\n",
    "\n",
    "    @staticmethod\n",
    "    def randomColor(image, label):\n",
    "\n",
    "        random_factor = np.random.randint(0, 31) / 10.\n",
    "        color_image = ImageEnhance.Color(image).enhance(random_factor)\n",
    "        random_factor = np.random.randint(10, 21) / 10.\n",
    "        brightness_image = ImageEnhance.Brightness(color_image).enhance(random_factor)\n",
    "        random_factor = np.random.randint(10, 21) / 10.\n",
    "        contrast_image = ImageEnhance.Contrast(brightness_image).enhance(random_factor)\n",
    "        random_factor = np.random.randint(0, 31) / 10.\n",
    "        return ImageEnhance.Sharpness(contrast_image).enhance(random_factor), label\n",
    "\n",
    "    @staticmethod\n",
    "    def randomGaussian(image, label, mean=0.2, sigma=0.3):\n",
    "\n",
    "        def gaussianNoisy(im, mean=0.2, sigma=0.3):\n",
    "\n",
    "            for _i in range(len(im)):\n",
    "                im[_i] += random.gauss(mean, sigma)\n",
    "            return im\n",
    "\n",
    "        img = np.array(image)\n",
    "        width, height = img.shape[:2]\n",
    "        img_r = gaussianNoisy(img[:, :, 0].flatten(), mean, sigma)\n",
    "        img_g = gaussianNoisy(img[:, :, 1].flatten(), mean, sigma)\n",
    "        img_b = gaussianNoisy(img[:, :, 2].flatten(), mean, sigma)\n",
    "        img[:, :, 0] = img_r.reshape([width, height])\n",
    "        img[:, :, 1] = img_g.reshape([width, height])\n",
    "        img[:, :, 2] = img_b.reshape([width, height])\n",
    "        return Image.fromarray(np.uint8(img)), label\n",
    "\n",
    "    @staticmethod\n",
    "    def saveImage(image, path):\n",
    "        # open_cv_image = np.array(image)\n",
    "        # open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
    "        # cv2.imwrite(open_cv_image,path)\n",
    "        image.save(path)\n",
    "\n",
    "\n",
    "def makeDir(path):\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            if not os.path.isfile(path):\n",
    "                # os.mkdir(path)\n",
    "                os.makedirs(path)\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except Exception:\n",
    "        print(str(Exception))\n",
    "\n",
    "\n",
    "\n",
    "def imageOps(func_name, image, label, img_des_path, label_des_path, img_file_name, label_file_name, times=3):\n",
    "    funcMap = {\"randomRotation\": DataAugmentation.randomRotation,\n",
    "               \"randomCrop\": DataAugmentation.randomCrop,\n",
    "               \"randomColor\": DataAugmentation.randomColor,\n",
    "               \"randomGaussian\": DataAugmentation.randomGaussian\n",
    "               }\n",
    "\n",
    "    funcshort = {\"randomRotation\": 'rr',\n",
    "               \"randomCrop\": 'rcr',\n",
    "               \"randomColor\": 'rc',\n",
    "               \"randomGaussian\": 'rg'\n",
    "               }\n",
    "\n",
    "    for _i in range(0, times, 1):\n",
    "        new_image, new_label = funcMap[func_name](image, label)\n",
    "        sname = funcshort[func_name]\n",
    "        DataAugmentation.saveImage(new_image, os.path.join(img_des_path, sname + str(_i) + img_file_name))\n",
    "        DataAugmentation.saveImage(new_label, os.path.join(label_des_path, sname + str(_i) + label_file_name))\n",
    "\n",
    "\n",
    "opsList = {\"randomRotation\", \"randomColor\", \"randomGaussian\"}\n",
    "\n",
    "\n",
    "def threadOPS(img_path, new_img_path, label_path, new_label_path):\n",
    "\n",
    "    img_names = os.listdir(img_path)\n",
    "    label_names = os.listdir(label_path)\n",
    "\n",
    "    n = len(img_names)\n",
    "\n",
    "    for i in range(n):\n",
    "        img_name = img_names[i]\n",
    "\n",
    "        label_name = label_names[i]\n",
    "\n",
    "\n",
    "        tmp_img_path = os.path.join(img_path, img_name)\n",
    "        tmp_label_path = os.path.join(label_path, label_name)\n",
    "\n",
    "        print(tmp_img_path)\n",
    "        image = DataAugmentation.openImage(tmp_img_path)\n",
    "\n",
    "        label = DataAugmentation.openImage(tmp_label_path)\n",
    "\n",
    "        threadImage = [0] * 5\n",
    "        _index = 0\n",
    "        for ops_name in opsList:\n",
    "            imageOps(ops_name,image, label, new_img_path, new_label_path, img_name,label_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threadOPS(r'C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\images',\n",
    "              r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\data\\train\\image\",\n",
    "              r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\data\\val\\label\",\n",
    "            r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\data\\val\\label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3426,
     "status": "ok",
     "timestamp": 1742630389230,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "RodQOG1-2sz6",
    "outputId": "6b3fd96c-a8fe-4f89-848e-ab3c7998eb45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow opencv-python numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337046,
     "status": "ok",
     "timestamp": 1742633466985,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "xK1PtXDQ3XfR",
    "outputId": "02777dcb-ee22-4381-8c5e-072ff929ce1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Step 2: Install Required Libraries\n",
    "!pip install pillow opencv-python numpy\n",
    "\n",
    "# ✅ Step 3: Import Necessary Libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import cv2\n",
    "\n",
    "# ✅ Step 4: Define Data Augmentation Class\n",
    "class DataAugmentation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def openImage(image_path):\n",
    "        return Image.open(image_path, mode=\"r\")\n",
    "\n",
    "    @staticmethod\n",
    "    def randomRotation(image, label, mode=Image.BICUBIC):\n",
    "        random_angle = np.random.randint(1, 360)\n",
    "        return image.rotate(random_angle, mode), label.rotate(random_angle, Image.NEAREST)\n",
    "\n",
    "    @staticmethod\n",
    "    def randomCrop(image, label):\n",
    "        image_width, image_height = image.size\n",
    "        crop_win_size = np.random.randint(40, 68)\n",
    "        random_region = (\n",
    "            (image_width - crop_win_size) >> 1,\n",
    "            (image_height - crop_win_size) >> 1,\n",
    "            (image_width + crop_win_size) >> 1,\n",
    "            (image_height + crop_win_size) >> 1,\n",
    "        )\n",
    "        return image.crop(random_region), label\n",
    "\n",
    "    @staticmethod\n",
    "    def randomColor(image, label):\n",
    "        random_factor = np.random.randint(0, 31) / 10.0\n",
    "        color_image = ImageEnhance.Color(image).enhance(random_factor)\n",
    "        random_factor = np.random.randint(10, 21) / 10.0\n",
    "        brightness_image = ImageEnhance.Brightness(color_image).enhance(random_factor)\n",
    "        random_factor = np.random.randint(10, 21) / 10.0\n",
    "        contrast_image = ImageEnhance.Contrast(brightness_image).enhance(random_factor)\n",
    "        random_factor = np.random.randint(0, 31) / 10.0\n",
    "        return ImageEnhance.Sharpness(contrast_image).enhance(random_factor), label\n",
    "\n",
    "    @staticmethod\n",
    "    def randomGaussian(image, label, mean=0.2, sigma=0.3):\n",
    "        def gaussianNoisy(im, mean=0.2, sigma=0.3):\n",
    "            for _i in range(len(im)):\n",
    "                im[_i] += random.gauss(mean, sigma)\n",
    "            return im\n",
    "\n",
    "        img = np.array(image)\n",
    "        width, height = img.shape[:2]\n",
    "        img_r = gaussianNoisy(img[:, :, 0].flatten(), mean, sigma)\n",
    "        img_g = gaussianNoisy(img[:, :, 1].flatten(), mean, sigma)\n",
    "        img_b = gaussianNoisy(img[:, :, 2].flatten(), mean, sigma)\n",
    "        img[:, :, 0] = img_r.reshape([width, height])\n",
    "        img[:, :, 1] = img_g.reshape([width, height])\n",
    "        img[:, :, 2] = img_b.reshape([width, height])\n",
    "        return Image.fromarray(np.uint8(img)), label\n",
    "\n",
    "    @staticmethod\n",
    "    def saveImage(image, path):\n",
    "        image.save(path)\n",
    "\n",
    "# ✅ Step 5: Create Directory Helper Function\n",
    "def makeDir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# ✅ Step 6: Define Image Processing Function\n",
    "def imageOps(func_name, image, label, img_des_path, label_des_path, img_file_name, label_file_name, times=3):\n",
    "    funcMap = {\n",
    "        \"randomRotation\": DataAugmentation.randomRotation,\n",
    "        \"randomCrop\": DataAugmentation.randomCrop,\n",
    "        \"randomColor\": DataAugmentation.randomColor,\n",
    "        \"randomGaussian\": DataAugmentation.randomGaussian,\n",
    "    }\n",
    "\n",
    "    funcshort = {\n",
    "        \"randomRotation\": 'rr',\n",
    "        \"randomCrop\": 'rcr',\n",
    "        \"randomColor\": 'rc',\n",
    "        \"randomGaussian\": 'rg',\n",
    "    }\n",
    "\n",
    "    for _i in range(times):\n",
    "        new_image, new_label = funcMap[func_name](image, label)\n",
    "        sname = funcshort[func_name]\n",
    "        DataAugmentation.saveImage(new_image, os.path.join(img_des_path, sname + str(_i) + img_file_name))\n",
    "        DataAugmentation.saveImage(new_label, os.path.join(label_des_path, sname + str(_i) + label_file_name))\n",
    "\n",
    "# ✅ Step 7: Define Multi-Threaded Image Processing Function\n",
    "def threadOPS(img_path, new_img_path, label_path, new_label_path):\n",
    "    makeDir(new_img_path)\n",
    "    makeDir(new_label_path)\n",
    "\n",
    "    img_names = sorted(os.listdir(img_path))\n",
    "    label_names = sorted(os.listdir(label_path))\n",
    "\n",
    "    for img_name, label_name in zip(img_names, label_names):\n",
    "        tmp_img_path = os.path.join(img_path, img_name)\n",
    "        tmp_label_path = os.path.join(label_path, label_name)\n",
    "\n",
    "        print(f\"Processing: {tmp_img_path}\")\n",
    "\n",
    "        image = DataAugmentation.openImage(tmp_img_path)\n",
    "        label = DataAugmentation.openImage(tmp_label_path)\n",
    "\n",
    "        opsList = {\"randomRotation\", \"randomColor\", \"randomGaussian\"}\n",
    "        for ops_name in opsList:\n",
    "            imageOps(ops_name, image, label, new_img_path, new_label_path, img_name, label_name)\n",
    "\n",
    "# ✅ Step 8: Set Paths to Your Dataset in Google Drive\n",
    "img_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\images\"\n",
    "new_img_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\data\\train\\image\"\n",
    "label_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\data\\train\\label\"\n",
    "new_label_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\data\\train\\label\"\n",
    "\n",
    "# ✅ Step 9: Run Image Processing\n",
    "threadOPS(img_path, new_img_path, label_path, new_label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13184,
     "status": "ok",
     "timestamp": 1742635925863,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "G4w1UEAT3fDA",
    "outputId": "c38c23b2-59e5-45ea-ad2d-e908dca26546",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖼️ Found 0 images: []\n",
      "🎭 Found 2243 masks: ['21_training_mask.gif', '22_training_mask.gif', '23_training_mask.gif', '24_training_mask.gif', '25_training_mask.gif']\n",
      "❌ No matching image-mask pairs found! Check filenames.\n",
      "✅ Matched 0 image-mask pairs.\n",
      "✅ Loaded 0 images and 0 masks.\n",
      "📏 Image shape: (0,), Mask shape: (0,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "\n",
    "# ✅ Define dataset directory paths (update if needed)\n",
    "IMAGE_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\images\"\n",
    "MASK_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\mask\"\n",
    "\n",
    "# ✅ Image dimensions (match model input size)\n",
    "IMG_SIZE = (512, 512)\n",
    "\n",
    "# ✅ Step 1: Check if dataset directories exist\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    print(f\"❌ Image directory does not exist: {IMAGE_DIR}\")\n",
    "if not os.path.exists(MASK_DIR):\n",
    "    print(f\"❌ Mask directory does not exist: {MASK_DIR}\")\n",
    "\n",
    "# ✅ Step 2: List and filter valid image/mask files\n",
    "def get_files(folder, valid_exts=('.png', '.jpg', '.jpeg', '.tif', '.tiff', '.gif')):\n",
    "    return sorted([f for f in os.listdir(folder) if f.lower().endswith(valid_exts)])\n",
    "\n",
    "image_filenames = get_files(IMAGE_DIR)\n",
    "mask_filenames = get_files(MASK_DIR)\n",
    "\n",
    "print(f\"🖼️ Found {len(image_filenames)} images: {image_filenames[:5]}\")\n",
    "print(f\"🎭 Found {len(mask_filenames)} masks: {mask_filenames[:5]}\")\n",
    "\n",
    "# ✅ Step 3: Match images and masks by filename prefix\n",
    "# Image files: e.g. \"21_training.tif\" → key: \"21_training\"\n",
    "image_dict = {os.path.splitext(f)[0]: f for f in image_filenames}\n",
    "# Mask files: e.g. \"21_training_mask.gif\" → remove the \"_mask\" suffix → key: \"21_training\"\n",
    "mask_dict = {os.path.splitext(f)[0].replace(\"_mask\", \"\"): f for f in mask_filenames}\n",
    "\n",
    "matched_images, matched_masks = [], []\n",
    "unmatched_images = []\n",
    "\n",
    "for img_prefix, img_name in image_dict.items():\n",
    "    mask_name = mask_dict.get(img_prefix, None)  # Look for a corresponding mask\n",
    "    if mask_name:\n",
    "        matched_images.append(img_name)\n",
    "        matched_masks.append(mask_name)\n",
    "    else:\n",
    "        unmatched_images.append(img_name)\n",
    "\n",
    "# ✅ Debugging: Report unmatched images\n",
    "if len(matched_images) == 0:\n",
    "    print(\"❌ No matching image-mask pairs found! Check filenames.\")\n",
    "    exit()\n",
    "\n",
    "for img_name in unmatched_images:\n",
    "    print(f\"⚠️ No matching mask found for: {img_name}\")\n",
    "\n",
    "print(f\"✅ Matched {len(matched_images)} image-mask pairs.\")\n",
    "\n",
    "# ✅ Step 4: Load and preprocess dataset\n",
    "def load_data(image_dir, mask_dir, image_files, mask_files):\n",
    "    images, masks = [], []\n",
    "\n",
    "    for img_name, mask_name in zip(image_files, mask_files):\n",
    "        # Construct full file paths using separate variable names\n",
    "        img_file_path = os.path.join(image_dir, img_name)\n",
    "        mask_file_path = os.path.join(mask_dir, mask_name)\n",
    "\n",
    "        # Check that both files exist\n",
    "        if not os.path.exists(img_file_path) or not os.path.exists(mask_file_path):\n",
    "            print(f\"❌ Missing file: {img_file_path} or {mask_file_path}\")\n",
    "            continue  # Skip if either file is missing\n",
    "\n",
    "        # Load and normalize image\n",
    "        img = load_img(img_file_path, target_size=IMG_SIZE)\n",
    "        img = img_to_array(img) / 255.0\n",
    "\n",
    "        # Load mask (grayscale), normalize, and threshold to binary\n",
    "        mask = load_img(mask_file_path, color_mode=\"grayscale\", target_size=IMG_SIZE)\n",
    "        mask = img_to_array(mask) / 255.0\n",
    "        mask = np.where(mask > 0.5, 1, 0)\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return np.array(images, dtype=np.float32), np.array(masks, dtype=np.float32)\n",
    "\n",
    "# ✅ Step 5: Load dataset using matched image and mask filenames\n",
    "X, Y = load_data(IMAGE_DIR, MASK_DIR, matched_images, matched_masks)\n",
    "\n",
    "print(f\"✅ Loaded {len(X)} images and {len(Y)} masks.\")\n",
    "print(f\"📏 Image shape: {X.shape}, Mask shape: {Y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5524,
     "status": "ok",
     "timestamp": 1742636567753,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "OiRtzYuWH4dG",
    "outputId": "dae0c6ba-76d5-4b01-af94-7aedcb4b1997",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow numpy pandas matplotlib scikit-learn opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742636902984,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "3-5RhBI8O7Zk"
   },
   "outputs": [],
   "source": [
    "# Define your paths\n",
    "dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\images\"  # Update with your dataset folder\n",
    "model_save_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\model\"  # Update for saving model\n",
    "# weights_path = \"/content/drive/MyDrive/pretrained_weights.h5\"  # Update if using pretrained weights\n",
    "output_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\results2.xlsx\"  # Folder to save outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1742636922285,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "_uo94LTcQOl1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1742636947559,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "rJyaPc39QTSW",
    "outputId": "1b682130-0016-4bc2-d517-acea5e9c4a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1742637023358,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "lG8d3MkOQZdA",
    "outputId": "fd2f6dd8-fb4f-470a-b5bc-7a422898c8a0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\images\n",
      "Contents: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "print(\"Contents:\", os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1742637112774,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "CTtKcKLgQr9s",
    "outputId": "fdc125d7-9041-42a4-bd70-328518cb6dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset restructured successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\images\"\n",
    "new_dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\organized\"\n",
    "\n",
    "class_1_path = os.path.join(new_dataset_path, \"class_1\")\n",
    "class_2_path = os.path.join(new_dataset_path, \"class_2\")\n",
    "\n",
    "# Create class folders\n",
    "os.makedirs(class_1_path, exist_ok=True)\n",
    "os.makedirs(class_2_path, exist_ok=True)\n",
    "\n",
    "# Move images into respective class folders (Modify logic based on actual labels)\n",
    "for file in os.listdir(dataset_path):\n",
    "    if \"class1\" in file:  # Adjust this condition based on your dataset\n",
    "        shutil.move(os.path.join(dataset_path, file), class_1_path)\n",
    "    else:\n",
    "        shutil.move(os.path.join(dataset_path, file), class_2_path)\n",
    "\n",
    "print(\"Dataset restructured successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97,
     "status": "ok",
     "timestamp": 1742637136752,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "xyOeN31HRByU",
    "outputId": "d6572096-803a-429d-8965-a8d570c53f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 690 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\organized\"\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91,
     "status": "ok",
     "timestamp": 1742637196954,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "jJSOZNEkRHpN",
    "outputId": "551c7938-320a-496c-9660-3a8421848286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\images\"\n",
    "file_list = os.listdir(dataset_path)\n",
    "\n",
    "print(f\"Total images found: {len(file_list)}\")\n",
    "print(file_list[:10])  # Print first 10 filenames for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1742637291471,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "n7m7OeBkRWV_",
    "outputId": "ae39b033-3765-4c4d-f574-05a46052f3c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder exists!\n",
      "Contents: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\images\"\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"✅ Folder exists!\")\n",
    "    print(\"Contents:\", os.listdir(dataset_path))\n",
    "else:\n",
    "    print(\"❌ Folder not found. Check path!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1742637406354,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "vG31bUe6R08O",
    "outputId": "06dec3c3-4f3e-42f0-a536-03ba5a2f2136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder exists!\n",
      "Contents: ['class_1', 'class_2']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\organized\"\n",
    "print(\"✅ Folder exists!\")\n",
    "print(\"Contents:\", os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1742637448142,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "DtWLagvZR-kC",
    "outputId": "54ca77b5-ad20-4e39-a62a-bf05f5c43c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 690 images belonging to 2 classes.\n",
      "Found 172 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\organized\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,  # ✅ Now points to the parent folder\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,  # ✅ Now points to the parent folder\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1742637547348,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "aAvGNCxXSTrh",
    "outputId": "a2934b31-cd39-4c54-bf12-a4372463df76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 862\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\organized\"\n",
    "\n",
    "total_images = sum([len(files) for _, _, files in os.walk(dataset_path)])\n",
    "print(\"Total images found:\", total_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1742637549915,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "u2OMv4IWSdxI",
    "outputId": "821eb055-9a99-498c-9af2-2c798bbd7b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'class_1' has 65 images.\n",
      "Class 'class_2' has 797 images.\n"
     ]
    }
   ],
   "source": [
    "for class_folder in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    if os.path.isdir(class_path):\n",
    "        print(f\"Class '{class_folder}' has {len(os.listdir(class_path))} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1742637568720,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "vx-XLOb-Sj8d",
    "outputId": "249c99ae-507f-40a7-dd0b-92b534be845f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1ffa92ae-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92af-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b0-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b1-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b2-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b3-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b4-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b5-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b6-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b7-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b8-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92b9-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92ba-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92bb-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92bc-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92bd-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92be-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92bf-8d87-11e8-9daf-6045cb817f5b..JPG', '1ffa92c0-8d87-11e8-9daf-6045cb817f5b..JPG', 'rr0_rc022_training.tif', 'rr0_rc023_training.tif', 'rr0_rc024_training.tif', 'rr0_rc026_training.tif', 'rr0_rc028_training.tif', 'rr0_rc029_training.tif', 'rr0_rc030_training.tif', 'rr0_rc031_training.tif', 'rr0_rc032_training.tif', 'rr0_rc033_training.tif', 'rr0_rc034_training.tif', 'rr0_rc035_training.tif', 'rr0_rc036_training.tif', 'rr0_rc037_training.tif', 'rr0_rc038_training.tif', 'rr0_rc039_training.tif', 'rr0_rc121_training.tif', 'rr0_rc122_training.tif', 'rr0_rc123_training.tif', 'rr0_rc124_training.tif', 'rr0_rc126_training.tif', 'rr0_rc127_training.tif', 'rr0_rc128_training.tif', 'rr0_rc129_training.tif', 'rr0_rc130_training.tif', 'rr0_rc131_training.tif', 'rr0_rc132_training.tif', 'rr0_rc133_training.tif', 'rr0_rc134_training.tif', 'rr0_rc138_training.tif', 'rr0_rc139_training.tif', 'rr0_rc140_training.tif', 'rr0_rc221_training.tif', 'rr0_rc222_training.tif', 'rr0_rc223_training.tif', 'rr0_rc224_training.tif', 'rr0_rc226_training.tif', 'rr0_rc227_training.tif', 'rr0_rc228_training.tif', 'rr0_rc229_training.tif', 'rr0_rc230_training.tif', 'rr0_rc231_training.tif', 'rr0_rc232_training.tif', 'rr0_rc233_training.tif', 'rr0_rc234_training.tif', 'rr0_rc235_training.tif']\n",
      "['21_training.tif', '22_training.tif', '23_training.tif', '24_training.tif', '25_training.tif', '26_training.tif', '27_training.tif', '28_training.tif', '29_training.tif', '30_training.tif', '31_training.tif', '32_training.tif', '33_training.tif', '34_training.tif', '35_training.tif', '36_training.tif', 'rg0_h23_training.tif', 'rg0_h24_training.tif', 'rg0_h26_training.tif', 'rg0_h27_training.tif', 'rg0_h28_training.tif', 'rg0_h29_training.tif', 'rg0_h30_training.tif', 'rg0_h31_training.tif', 'rg0_h32_training.tif', 'rg0_h33_training.tif', 'rg0_h34_training.tif', 'rg0_h35_training.tif', 'rg0_h36_training.tif', 'rg0_h37_training.tif', 'rg0_h38_training.tif', 'rg0_h39_training.tif', 'rg0_h40_training.tif', 'rg0_hv21_training.tif', 'rg0_hv22_training.tif', 'rg0_hv23_training.tif', 'rg0_hv24_training.tif', 'rg0_hv26_training.tif', 'rg0_hv27_training.tif', 'rg0_hv28_training.tif', 'rg0_hv29_training.tif', 'rg0_hv30_training.tif', 'rg0_hv31_training.tif', 'rg0_hv32_training.tif', 'rg0_hv33_training.tif', 'rg0_hv34_training.tif', 'rg0_hv35_training.tif', 'rg0_hv36_training.tif', 'rg0_hv37_training.tif', 'rg0_hv38_training.tif', 'rg0_hv39_training.tif', 'rg0_hv40_training.tif', 'rg0_o21_training.tif', 'rg0_o22_training.tif', 'rg0_o23_training.tif', 'rg0_o24_training.tif', 'rg0_o26_training.tif', 'rg0_o27_training.tif', 'rg0_o28_training.tif', 'rg0_o29_training.tif', 'rg0_o30_training.tif', 'rg0_o31_training.tif', 'rg0_o32_training.tif', 'rg0_o33_training.tif', 'rg0_o34_training.tif', 'rg0_o35_training.tif', 'rg0_o36_training.tif', 'rg0_o37_training.tif', 'rg0_o38_training.tif', 'rg0_o39_training.tif', 'rg0_o40_training.tif', 'rg0_rc021_training.tif', 'rg0_rg021_training.tif', 'rg0_rg022_training.tif', 'rg0_rg023_training.tif', 'rg0_rg024_training.tif', 'rg0_rg026_training.tif', 'rg0_rg027_training.tif', 'rg0_rg028_training.tif', 'rg0_rg029_training.tif', 'rg0_rg030_training.tif', 'rg0_rg031_training.tif', 'rg0_rg032_training.tif', 'rg0_rg033_training.tif', 'rg0_rg034_training.tif', 'rg0_rg035_training.tif', 'rg0_rg036_training.tif', 'rg0_rg037_training.tif', 'rg0_rg038_training.tif', 'rg0_rg039_training.tif', 'rg0_rg040_training.tif', 'rg0_rg121_training.tif', 'rg0_rg122_training.tif', 'rg0_rg123_training.tif', 'rg0_rg124_training.tif', 'rg0_rg126_training.tif', 'rg0_rg127_training.tif', 'rg0_rg128_training.tif', 'rg0_rg129_training.tif', 'rg0_rg130_training.tif', 'rg0_rg131_training.tif', 'rg0_rg132_training.tif', 'rg0_rg133_training.tif', 'rg0_rg134_training.tif', 'rg0_rg135_training.tif', 'rg0_rg136_training.tif', 'rg0_rg137_training.tif', 'rg0_rg138_training.tif', 'rg0_rg139_training.tif', 'rg0_rg140_training.tif', 'rg0_rg221_training.tif', 'rg0_rg222_training.tif', 'rg0_rg223_training.tif', 'rg0_rg224_training.tif', 'rg0_rg226_training.tif', 'rg0_rg227_training.tif', 'rg0_rg228_training.tif', 'rg0_rg229_training.tif', 'rg0_rg230_training.tif', 'rg0_rg231_training.tif', 'rg0_rg232_training.tif', 'rg0_rg233_training.tif', 'rg0_rg234_training.tif', 'rg0_rg235_training.tif', 'rg0_rg236_training.tif', 'rg0_rg237_training.tif', 'rg0_rg238_training.tif', 'rg0_rg239_training.tif', 'rg0_rg240_training.tif', 'rg0_rr021_training.tif', 'rg0_rr022_training.tif', 'rg0_rr023_training.tif', 'rg0_rr024_training.tif', 'rg0_rr026_training.tif', 'rg0_rr027_training.tif', 'rg0_rr028_training.tif', 'rg0_rr029_training.tif', 'rg0_rr030_training.tif', 'rg0_rr031_training.tif', 'rg0_rr032_training.tif', 'rg0_rr033_training.tif', 'rg0_rr034_training.tif', 'rg0_rr035_training.tif', 'rg0_rr036_training.tif', 'rg0_rr037_training.tif', 'rg0_rr038_training.tif', 'rg0_rr039_training.tif', 'rg0_rr040_training.tif', 'rg0_rr121_training.tif', 'rg0_rr122_training.tif', 'rg0_rr123_training.tif', 'rg0_rr124_training.tif', 'rg0_rr126_training.tif', 'rg0_rr127_training.tif', 'rg0_rr128_training.tif', 'rg0_rr129_training.tif', 'rg0_rr130_training.tif', 'rg0_rr131_training.tif', 'rg0_rr132_training.tif', 'rg0_rr133_training.tif', 'rg0_rr134_training.tif', 'rg0_rr135_training.tif', 'rg0_rr136_training.tif', 'rg0_rr137_training.tif', 'rg0_rr138_training.tif', 'rg0_rr139_training.tif', 'rg0_rr140_training.tif', 'rg0_rr221_training.tif', 'rg0_rr222_training.tif', 'rg0_rr223_training.tif', 'rg0_rr224_training.tif', 'rg0_rr226_training.tif', 'rg0_rr227_training.tif', 'rg0_rr228_training.tif', 'rg0_rr229_training.tif', 'rg0_rr230_training.tif', 'rg0_rr231_training.tif', 'rg0_rr232_training.tif', 'rg0_rr233_training.tif', 'rg0_rr234_training.tif', 'rg0_rr235_training.tif', 'rg0_rr236_training.tif', 'rg0_rr237_training.tif', 'rg0_rr238_training.tif', 'rg0_rr239_training.tif', 'rg0_rr240_training.tif', 'rg0_v21_training.tif', 'rg0_v22_training.tif', 'rg0_v23_training.tif', 'rg0_v24_training.tif', 'rg0_v26_training.tif', 'rg0_v27_training.tif', 'rg0_v28_training.tif', 'rg0_v29_training.tif', 'rg0_v30_training.tif', 'rg0_v31_training.tif', 'rg0_v32_training.tif', 'rg0_v33_training.tif', 'rg0_v34_training.tif', 'rg0_v35_training.tif', 'rg0_v36_training.tif', 'rg0_v37_training.tif', 'rg0_v38_training.tif', 'rg0_v39_training.tif', 'rg0_v40_training.tif', 'rg1_h21_training.tif', 'rg1_h22_training.tif', 'rg1_h23_training.tif', 'rg1_h24_training.tif', 'rg1_h26_training.tif', 'rg1_h27_training.tif', 'rg1_h28_training.tif', 'rg1_h29_training.tif', 'rg1_h30_training.tif', 'rg1_h31_training.tif', 'rg1_h32_training.tif', 'rg1_h33_training.tif', 'rg1_h34_training.tif', 'rg1_h35_training.tif', 'rg1_h36_training.tif', 'rg1_h37_training.tif', 'rg1_h38_training.tif', 'rg1_h39_training.tif', 'rg1_h40_training.tif', 'rg1_hv21_training.tif', 'rg1_hv22_training.tif', 'rg1_hv23_training.tif', 'rg1_hv24_training.tif', 'rg1_hv26_training.tif', 'rg1_hv27_training.tif', 'rg1_hv28_training.tif', 'rg1_hv29_training.tif', 'rg1_hv30_training.tif', 'rg1_hv31_training.tif', 'rg1_hv32_training.tif', 'rg1_hv33_training.tif', 'rg1_hv34_training.tif', 'rg1_hv35_training.tif', 'rg1_hv36_training.tif', 'rg1_hv37_training.tif', 'rg1_hv38_training.tif', 'rg1_hv39_training.tif', 'rg1_hv40_training.tif', 'rg1_o21_training.tif', 'rg1_o22_training.tif', 'rg1_o23_training.tif', 'rg1_o24_training.tif', 'rg1_o26_training.tif', 'rg1_o27_training.tif', 'rg1_o28_training.tif', 'rg1_o29_training.tif', 'rg1_o30_training.tif', 'rg1_o31_training.tif', 'rg1_o32_training.tif', 'rg1_o33_training.tif', 'rg1_o34_training.tif', 'rg1_o35_training.tif', 'rg1_o36_training.tif', 'rg1_o37_training.tif', 'rg1_o38_training.tif', 'rg1_o39_training.tif', 'rg1_o40_training.tif', 'rg1_rc021_training.tif', 'rg1_rc022_training.tif', 'rg1_rg022_training.tif', 'rg1_rg023_training.tif', 'rg1_rg024_training.tif', 'rg1_rg026_training.tif', 'rg1_rg027_training.tif', 'rg1_rg028_training.tif', 'rg1_rg029_training.tif', 'rg1_rg030_training.tif', 'rg1_rg031_training.tif', 'rg1_rg032_training.tif', 'rg1_rg033_training.tif', 'rg1_rg034_training.tif', 'rg1_rg035_training.tif', 'rg1_rg036_training.tif', 'rg1_rg037_training.tif', 'rg1_rg038_training.tif', 'rg1_rg039_training.tif', 'rg1_rg040_training.tif', 'rg1_rg121_training.tif', 'rg1_rg122_training.tif', 'rg1_rg123_training.tif', 'rg1_rg124_training.tif', 'rg1_rg126_training.tif', 'rg1_rg127_training.tif', 'rg1_rg128_training.tif', 'rg1_rg129_training.tif', 'rg1_rg130_training.tif', 'rg1_rg131_training.tif', 'rg1_rg132_training.tif', 'rg1_rg133_training.tif', 'rg1_rg134_training.tif', 'rg1_rg135_training.tif', 'rg1_rg136_training.tif', 'rg1_rg137_training.tif', 'rg1_rg138_training.tif', 'rg1_rg139_training.tif', 'rg1_rg140_training.tif', 'rg1_rg221_training.tif', 'rg1_rg222_training.tif', 'rg1_rg223_training.tif', 'rg1_rg224_training.tif', 'rg1_rg226_training.tif', 'rg1_rg227_training.tif', 'rg1_rg228_training.tif', 'rg1_rg229_training.tif', 'rg1_rg230_training.tif', 'rg1_rg231_training.tif', 'rg1_rg232_training.tif', 'rg1_rg233_training.tif', 'rg1_rg234_training.tif', 'rg1_rg235_training.tif', 'rg1_rg236_training.tif', 'rg1_rg237_training.tif', 'rg1_rg238_training.tif', 'rg1_rg239_training.tif', 'rg1_rg240_training.tif', 'rg1_rr021_training.tif', 'rg1_rr022_training.tif', 'rg1_rr023_training.tif', 'rg1_rr024_training.tif', 'rg1_rr026_training.tif', 'rg1_rr027_training.tif', 'rg1_rr028_training.tif', 'rg1_rr029_training.tif', 'rg1_rr030_training.tif', 'rg1_rr031_training.tif', 'rg1_rr032_training.tif', 'rg1_rr033_training.tif', 'rg1_rr034_training.tif', 'rg1_rr035_training.tif', 'rg1_rr036_training.tif', 'rg1_rr037_training.tif', 'rg1_rr038_training.tif', 'rg1_rr039_training.tif', 'rg1_rr040_training.tif', 'rg1_rr121_training.tif', 'rg1_rr122_training.tif', 'rg1_rr123_training.tif', 'rg1_rr124_training.tif', 'rg1_rr126_training.tif', 'rg1_rr127_training.tif', 'rg1_rr128_training.tif', 'rg1_rr129_training.tif', 'rg1_rr130_training.tif', 'rg1_rr131_training.tif', 'rg1_rr132_training.tif', 'rg1_rr133_training.tif', 'rg1_rr134_training.tif', 'rg1_rr135_training.tif', 'rg1_rr136_training.tif', 'rg1_rr137_training.tif', 'rg1_rr138_training.tif', 'rg1_rr139_training.tif', 'rg1_rr140_training.tif', 'rg1_rr221_training.tif', 'rg1_rr222_training.tif', 'rg1_rr223_training.tif', 'rg1_rr224_training.tif', 'rg1_rr226_training.tif', 'rg1_rr227_training.tif', 'rg1_rr228_training.tif', 'rg1_rr229_training.tif', 'rg1_rr230_training.tif', 'rg1_rr231_training.tif', 'rg1_rr232_training.tif', 'rg1_rr233_training.tif', 'rg1_rr234_training.tif', 'rg1_rr235_training.tif', 'rg1_rr236_training.tif', 'rg1_rr237_training.tif', 'rg1_rr238_training.tif', 'rg1_rr239_training.tif', 'rg1_rr240_training.tif', 'rg1_v21_training.tif', 'rg1_v22_training.tif', 'rg1_v23_training.tif', 'rg1_v24_training.tif', 'rg1_v26_training.tif', 'rg1_v27_training.tif', 'rg1_v28_training.tif', 'rg1_v29_training.tif', 'rg1_v30_training.tif', 'rg1_v31_training.tif', 'rg1_v32_training.tif', 'rg1_v33_training.tif', 'rg1_v34_training.tif', 'rg1_v35_training.tif', 'rg1_v36_training.tif', 'rg1_v37_training.tif', 'rg1_v38_training.tif', 'rg1_v39_training.tif', 'rg1_v40_training.tif', 'rg2_h21_training.tif', 'rg2_h22_training.tif', 'rg2_h23_training.tif', 'rg2_h24_training.tif', 'rg2_h26_training.tif', 'rg2_h27_training.tif', 'rg2_h28_training.tif', 'rg2_h29_training.tif', 'rg2_h30_training.tif', 'rg2_h31_training.tif', 'rg2_h32_training.tif', 'rg2_h33_training.tif', 'rg2_h34_training.tif', 'rg2_h35_training.tif', 'rg2_h36_training.tif', 'rg2_h37_training.tif', 'rg2_h38_training.tif', 'rg2_h39_training.tif', 'rg2_h40_training.tif', 'rg2_hv21_training.tif', 'rg2_hv22_training.tif', 'rg2_hv23_training.tif', 'rg2_hv24_training.tif', 'rg2_hv26_training.tif', 'rg2_hv27_training.tif', 'rg2_hv28_training.tif', 'rg2_hv29_training.tif', 'rg2_hv30_training.tif', 'rg2_hv31_training.tif', 'rg2_hv32_training.tif', 'rg2_hv33_training.tif', 'rg2_hv34_training.tif', 'rg2_hv35_training.tif', 'rg2_hv36_training.tif', 'rg2_hv37_training.tif', 'rg2_hv38_training.tif', 'rg2_hv39_training.tif', 'rg2_hv40_training.tif', 'rg2_o21_training.tif', 'rg2_o22_training.tif', 'rg2_o23_training.tif', 'rg2_o24_training.tif', 'rg2_o26_training.tif', 'rg2_o27_training.tif', 'rg2_o28_training.tif', 'rg2_o29_training.tif', 'rg2_o30_training.tif', 'rg2_o31_training.tif', 'rg2_o32_training.tif', 'rg2_o33_training.tif', 'rg2_o34_training.tif', 'rg2_o35_training.tif', 'rg2_o36_training.tif', 'rg2_o37_training.tif', 'rg2_o38_training.tif', 'rg2_o39_training.tif', 'rg2_o40_training.tif', 'rg2_rc021_training.tif', 'rg2_rc022_training.tif', 'rg2_rc023_training.tif', 'rg2_rc235_training.tif', 'rg2_rc236_training.tif', 'rg2_rc237_training.tif', 'rg2_rc238_training.tif', 'rg2_rc239_training.tif', 'rg2_rc240_training.tif', 'rg2_rg021_training.tif', 'rg2_rg022_training.tif', 'rg2_rg023_training.tif', 'rg2_rg024_training.tif', 'rg2_rg026_training.tif', 'rg2_rg027_training.tif', 'rg2_rg028_training.tif', 'rg2_rg029_training.tif', 'rg2_rg030_training.tif', 'rg2_rg031_training.tif', 'rg2_rg032_training.tif', 'rg2_rg033_training.tif', 'rg2_rg034_training.tif', 'rg2_rg035_training.tif', 'rg2_rg036_training.tif', 'rg2_rg037_training.tif', 'rg2_rg038_training.tif', 'rg2_rg039_training.tif', 'rg2_rg040_training.tif', 'rg2_rg121_training.tif', 'rg2_rg122_training.tif', 'rg2_rg123_training.tif', 'rg2_rg124_training.tif', 'rg2_rg126_training.tif', 'rg2_rg127_training.tif', 'rg2_rg128_training.tif', 'rg2_rg129_training.tif', 'rg2_rg130_training.tif', 'rg2_rg131_training.tif', 'rg2_rg132_training.tif', 'rg2_rg133_training.tif', 'rg2_rg134_training.tif', 'rg2_rg135_training.tif', 'rg2_rg136_training.tif', 'rg2_rg137_training.tif', 'rg2_rg138_training.tif', 'rg2_rg139_training.tif', 'rg2_rg140_training.tif', 'rg2_rg221_training.tif', 'rg2_rg222_training.tif', 'rg2_rg223_training.tif', 'rg2_rg224_training.tif', 'rg2_rg226_training.tif', 'rg2_rg227_training.tif', 'rg2_rg228_training.tif', 'rg2_rg229_training.tif', 'rg2_rg230_training.tif', 'rg2_rg231_training.tif', 'rg2_rg232_training.tif', 'rg2_rg233_training.tif', 'rg2_rg234_training.tif', 'rg2_rg235_training.tif', 'rg2_rg236_training.tif', 'rg2_rg237_training.tif', 'rg2_rg238_training.tif', 'rg2_rg239_training.tif', 'rg2_rg240_training.tif', 'rg2_rr021_training.tif', 'rg2_rr022_training.tif', 'rg2_rr023_training.tif', 'rg2_rr024_training.tif', 'rg2_rr026_training.tif', 'rg2_rr027_training.tif', 'rg2_rr028_training.tif', 'rg2_rr029_training.tif', 'rg2_rr030_training.tif', 'rg2_rr031_training.tif', 'rg2_rr032_training.tif', 'rg2_rr033_training.tif', 'rg2_rr034_training.tif', 'rg2_rr035_training.tif', 'rg2_rr036_training.tif', 'rg2_rr037_training.tif', 'rg2_rr038_training.tif', 'rg2_rr039_training.tif', 'rg2_rr040_training.tif', 'rg2_rr121_training.tif', 'rg2_rr122_training.tif', 'rg2_rr123_training.tif', 'rg2_rr124_training.tif', 'rg2_rr126_training.tif', 'rg2_rr127_training.tif', 'rg2_rr128_training.tif', 'rg2_rr129_training.tif', 'rg2_rr130_training.tif', 'rg2_rr131_training.tif', 'rg2_rr132_training.tif', 'rg2_rr133_training.tif', 'rg2_rr134_training.tif', 'rg2_rr135_training.tif', 'rg2_rr136_training.tif', 'rg2_rr137_training.tif', 'rg2_rr138_training.tif', 'rg2_rr139_training.tif', 'rg2_rr140_training.tif', 'rg2_rr221_training.tif', 'rg2_rr222_training.tif', 'rg2_rr223_training.tif', 'rg2_rr224_training.tif', 'rg2_rr226_training.tif', 'rg2_rr227_training.tif', 'rg2_rr228_training.tif', 'rg2_rr229_training.tif', 'rg2_rr230_training.tif', 'rg2_rr231_training.tif', 'rg2_rr232_training.tif', 'rg2_rr233_training.tif', 'rg2_rr234_training.tif', 'rg2_rr235_training.tif', 'rg2_rr236_training.tif', 'rg2_rr237_training.tif', 'rg2_rr238_training.tif', 'rg2_rr239_training.tif', 'rg2_rr240_training.tif', 'rg2_v21_training.tif', 'rg2_v22_training.tif', 'rg2_v23_training.tif', 'rg2_v24_training.tif', 'rg2_v26_training.tif', 'rg2_v27_training.tif', 'rg2_v28_training.tif', 'rg2_v29_training.tif', 'rg2_v30_training.tif', 'rg2_v31_training.tif', 'rg2_v32_training.tif', 'rg2_v33_training.tif', 'rg2_v34_training.tif', 'rg2_v35_training.tif', 'rg2_v36_training.tif', 'rg2_v37_training.tif', 'rg2_v38_training.tif', 'rg2_v39_training.tif', 'rg2_v40_training.tif', 'rr0_h21_training.tif', 'rr0_h22_training.tif', 'rr0_h23_training.tif', 'rr0_h24_training.tif', 'rr0_h26_training.tif', 'rr0_h27_training.tif', 'rr0_h28_training.tif', 'rr0_h29_training.tif', 'rr0_h30_training.tif', 'rr0_h31_training.tif', 'rr0_h32_training.tif', 'rr0_h33_training.tif', 'rr0_h34_training.tif', 'rr0_h35_training.tif', 'rr0_h36_training.tif', 'rr0_h37_training.tif', 'rr0_h38_training.tif', 'rr0_h39_training.tif', 'rr0_h40_training.tif', 'rr0_hv21_training.tif', 'rr0_hv22_training.tif', 'rr0_hv23_training.tif', 'rr0_hv24_training.tif', 'rr0_hv26_training.tif', 'rr0_hv27_training.tif', 'rr0_hv28_training.tif', 'rr0_hv29_training.tif', 'rr0_hv30_training.tif', 'rr0_hv31_training.tif', 'rr0_hv32_training.tif', 'rr0_hv33_training.tif', 'rr0_hv34_training.tif', 'rr0_hv35_training.tif', 'rr0_hv36_training.tif', 'rr0_hv37_training.tif', 'rr0_hv38_training.tif', 'rr0_hv39_training.tif', 'rr0_hv40_training.tif', 'rr0_o21_training.tif', 'rr0_o22_training.tif', 'rr0_o23_training.tif', 'rr0_o24_training.tif', 'rr0_o26_training.tif', 'rr0_o27_training.tif', 'rr0_o28_training.tif', 'rr0_o29_training.tif', 'rr0_o30_training.tif', 'rr0_o31_training.tif', 'rr0_o32_training.tif', 'rr0_o33_training.tif', 'rr0_o34_training.tif', 'rr0_o35_training.tif', 'rr0_o36_training.tif', 'rr0_rc236_training.tif', 'rr0_rc237_training.tif', 'rr0_rc238_training.tif', 'rr0_rc239_training.tif', 'rr0_rc240_training.tif', 'rr0_rg021_training.tif', 'rr0_rg022_training.tif', 'rr0_rg023_training.tif', 'rr0_rg024_training.tif', 'rr0_rg026_training.tif', 'rr0_rg027_training.tif', 'rr0_rg028_training.tif', 'rr0_rg029_training.tif', 'rr0_rg030_training.tif', 'rr0_rg031_training.tif', 'rr0_rg032_training.tif', 'rr0_rg033_training.tif', 'rr0_rg034_training.tif', 'rr0_rg035_training.tif', 'rr0_rg036_training.tif', 'rr0_rg037_training.tif', 'rr0_rg038_training.tif', 'rr0_rg039_training.tif', 'rr0_rg040_training.tif', 'rr0_rg121_training.tif', 'rr0_rg122_training.tif', 'rr0_rg123_training.tif', 'rr0_rg124_training.tif', 'rr0_rg126_training.tif', 'rr0_rg127_training.tif', 'rr0_rg128_training.tif', 'rr0_rg129_training.tif', 'rr0_rg130_training.tif', 'rr0_rg131_training.tif', 'rr0_rg132_training.tif', 'rr0_rg133_training.tif', 'rr0_rg134_training.tif', 'rr0_rg135_training.tif', 'rr0_rg136_training.tif', 'rr0_rg137_training.tif', 'rr0_rg138_training.tif', 'rr0_rg139_training.tif', 'rr0_rg140_training.tif', 'rr0_rg221_training.tif', 'rr0_rg222_training.tif', 'rr0_rg223_training.tif', 'rr0_rg224_training.tif', 'rr0_rg226_training.tif', 'rr0_rg227_training.tif', 'rr0_rg228_training.tif', 'rr0_rg229_training.tif', 'rr0_rg230_training.tif', 'rr0_rg231_training.tif', 'rr0_rg232_training.tif', 'rr0_rg233_training.tif', 'rr0_rg234_training.tif', 'rr0_rg235_training.tif', 'rr0_rg236_training.tif', 'rr0_rg237_training.tif', 'rr0_rg238_training.tif', 'rr0_rg239_training.tif', 'rr0_rg240_training.tif', 'rr0_rr021_training.tif', 'rr0_rr022_training.tif', 'rr0_rr023_training.tif', 'rr0_rr024_training.tif', 'rr0_rr026_training.tif', 'rr0_rr027_training.tif', 'rr0_rr028_training.tif', 'rr0_rr029_training.tif', 'rr0_rr030_training.tif', 'rr0_rr031_training.tif', 'rr0_rr032_training.tif', 'rr0_rr033_training.tif', 'rr0_rr034_training.tif', 'rr0_rr035_training.tif', 'rr0_rr036_training.tif', 'rr0_rr037_training.tif', 'rr0_rr038_training.tif', 'rr0_rr039_training.tif', 'rr0_rr040_training.tif', 'rr0_rr121_training.tif', 'rr0_rr122_training.tif', 'rr0_rr123_training.tif', 'rr0_rr124_training.tif', 'rr0_rr126_training.tif', 'rr0_rr127_training.tif', 'rr0_rr128_training.tif', 'rr0_rr129_training.tif', 'rr0_rr130_training.tif', 'rr0_rr131_training.tif', 'rr0_rr132_training.tif', 'rr0_rr133_training.tif', 'rr0_rr134_training.tif', 'rr0_rr135_training.tif', 'rr0_rr136_training.tif', 'rr0_rr137_training.tif', 'rr0_rr138_training.tif', 'rr0_rr139_training.tif', 'rr0_rr140_training.tif', 'rr0_rr221_training.tif', 'rr0_rr222_training.tif', 'rr0_rr223_training.tif', 'rr0_rr224_training.tif', 'rr0_rr226_training.tif', 'rr0_rr227_training.tif', 'rr0_rr228_training.tif', 'rr0_rr229_training.tif', 'rr0_rr230_training.tif', 'rr0_rr231_training.tif', 'rr0_rr232_training.tif', 'rr0_rr233_training.tif', 'rr0_rr234_training.tif', 'rr0_rr235_training.tif', 'rr0_rr236_training.tif', 'rr0_rr237_training.tif', 'rr0_rr238_training.tif', 'rr0_rr239_training.tif', 'rr0_rr240_training.tif', 'rr0_v21_training.tif', 'rr0_v22_training.tif', 'rr0_v23_training.tif', 'rr0_v24_training.tif', 'rr0_v26_training.tif', 'rr0_v27_training.tif', 'rr0_v28_training.tif', 'rr0_v29_training.tif', 'rr0_v30_training.tif', 'rr0_v31_training.tif', 'rr0_v32_training.tif', 'rr0_v33_training.tif', 'rr0_v34_training.tif', 'rr0_v35_training.tif', 'rr0_v36_training.tif', 'rr0_v37_training.tif', 'rr0_v38_training.tif', 'rr0_v39_training.tif', 'rr0_v40_training.tif', 'rr1_h21_training.tif', 'rr1_h22_training.tif', 'rr1_h23_training.tif', 'rr1_h24_training.tif', 'rr1_h26_training.tif', 'rr1_h27_training.tif', 'rr1_h28_training.tif', 'rr1_h29_training.tif', 'rr1_h30_training.tif', 'rr1_h31_training.tif', 'rr1_h32_training.tif']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(dataset_path + \"/class_1\"))  # Check inside class folders\n",
    "print(os.listdir(dataset_path + \"/class_2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 138,
     "status": "ok",
     "timestamp": 1742637587940,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "nHLXn4OYSxGs",
    "outputId": "54ee56f0-ca8d-4f96-b134-f2584413c7ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24196\\1336093228.py:1: DeprecationWarning: 'imghdr' is deprecated and slated for removal in Python 3.13\n",
      "  import imghdr\n"
     ]
    }
   ],
   "source": [
    "import imghdr\n",
    "\n",
    "for class_folder in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    for file in os.listdir(class_path):\n",
    "        file_path = os.path.join(class_path, file)\n",
    "        if not imghdr.what(file_path):  # Check if it's an image\n",
    "            print(f\"❌ Non-image file found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1742637627293,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "QFvOQz4-S1yS"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "for class_folder in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    if os.path.isdir(class_path):\n",
    "        for file in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.verify()  # Check if it's a valid image\n",
    "            except (IOError, SyntaxError):\n",
    "                print(f\"❌ Corrupt or invalid image: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1742637672291,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "qPqKFJmNS_Zx",
    "outputId": "eca2cc6a-70e0-4c13-953d-0480488fe160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 690 images belonging to 2 classes.\n",
      "Found 172 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(256, 256),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1742637702334,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "GwCA-r_pTKY7",
    "outputId": "eee79672-9b72-45bf-c26a-781b326106bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_1': 0, 'class_2': 1}\n",
      "690\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)  # Shows assigned class labels\n",
    "print(train_generator.samples)  # Number of training images\n",
    "print(val_generator.samples)  # Number of validation images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1742637742950,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "WHhCsWQITRtv",
    "outputId": "b221f7bf-1d4a-451a-e5d7-74a8e7b9c38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted files: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "corrupted_files = []\n",
    "\n",
    "for class_folder in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_folder)\n",
    "    if os.path.isdir(class_path):  # Ensure it's a directory\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path)  # Try opening the image\n",
    "                img.verify()  # Verify if image is not corrupted\n",
    "            except Exception as e:\n",
    "                corrupted_files.append(img_path)\n",
    "\n",
    "print(\"Corrupted files:\", corrupted_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">246016</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">31,490,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m246016\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m31,490,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,509,826</span> (120.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,509,826\u001b[0m (120.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,509,826</span> (120.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,509,826\u001b[0m (120.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Define a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(2, activation='softmax')  # 2 classes (class_1, class_2)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1742637802963,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "xDeqG205TboA",
    "outputId": "5f6fddcd-2642-4ce1-d0f5-b160c2128631"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36144,
     "status": "ok",
     "timestamp": 1742637862736,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "S5I3xXTyThs1",
    "outputId": "348213e8-fbc2-43f5-f8b4-a92341cb348f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1s/step - accuracy: 0.9045 - loss: 3.1783 - val_accuracy: 0.9244 - val_loss: 0.1387\n",
      "Epoch 2/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9740 - loss: 0.1107 - val_accuracy: 0.9244 - val_loss: 0.1292\n",
      "Epoch 3/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9745 - loss: 0.1324 - val_accuracy: 0.9767 - val_loss: 0.0482\n",
      "Epoch 4/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9869 - loss: 0.0380 - val_accuracy: 0.9767 - val_loss: 0.0634\n",
      "Epoch 5/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9831 - loss: 0.0562 - val_accuracy: 0.9826 - val_loss: 0.0459\n",
      "Epoch 6/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9874 - loss: 0.0457 - val_accuracy: 0.9826 - val_loss: 0.0433\n",
      "Epoch 7/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9919 - loss: 0.0411 - val_accuracy: 0.9826 - val_loss: 0.0552\n",
      "Epoch 8/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9889 - loss: 0.0281 - val_accuracy: 0.9826 - val_loss: 0.0396\n",
      "Epoch 9/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9945 - loss: 0.0210 - val_accuracy: 0.9884 - val_loss: 0.0345\n",
      "Epoch 10/10\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0180 - val_accuracy: 0.9826 - val_loss: 0.0486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fa8ce584d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, validation_data=val_generator, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1742637908142,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "UsXlEAlYTwFX",
    "outputId": "40577036-5fc6-4489-c836-78b665ce90f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 498ms/step - accuracy: 0.9895 - loss: 0.0327\n",
      "Validation Accuracy: 0.9826\n",
      "Validation Loss: 0.0486\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2588,
     "status": "ok",
     "timestamp": 1742637927776,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "ZooNNy80UDzt",
    "outputId": "8f8e74b1-3c72-472d-ae0e-9206f7971704"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.save(r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\model\\model.h5\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1461,
     "status": "ok",
     "timestamp": 1742638019655,
     "user": {
      "displayName": "Kalpana Gurnani",
      "userId": "09822202568531146923"
     },
     "user_tz": -330
    },
    "id": "FoeN_vcYUIJ8",
    "outputId": "63b8d198-adc9-48cc-d2a6-82f19df093b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "Predicted Class: class_1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(img_path, model):\n",
    "    img = image.load_img(img_path, target_size=(256, 256))\n",
    "    img_array = image.img_to_array(img) / 255.0  # Normalize\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Expand batch dimension\n",
    "\n",
    "    prediction = model.predict(img_array)\n",
    "    class_idx = np.argmax(prediction)\n",
    "    return \"class_1\" if class_idx == 0 else \"class_2\"\n",
    "\n",
    "#test_img = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\organized\\class_1\\1ffa92b8-8d87-11e8-9daf-6045cb817f5b..JPG\"\n",
    "test_img = r\"C:\\Users\\HP\\OneDrive\\Desktop - Copy\\New folder\\tortuosity-master\\tortuosity-master\\datasets\\train\\organized\\class_2\\21_training.tif\"\n",
    "print(\"Predicted Class:\", predict_image(test_img, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umPseEDlUe24"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKpEZcC1jwmalxdeSmR4V/",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
